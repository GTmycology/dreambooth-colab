{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmCPmqFL6hCQ"
      },
      "source": [
        "# ⭐ Lora Trainer by Hollowstrawberry\n",
        "\n",
        "This is based on the work of [Kohya-ss](https://github.com/kohya-ss/sd-scripts) and [Linaqruf](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb). Thank you!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "owDA9dkPTV-5",
        "outputId": "50d1c914-7246-4998-c837-161b5863ee35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Oct 12 07:23:19 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "qqSHRokdDY0s",
        "outputId": "106980e5-a0e4-44e4-b933-1f5a627b1314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "✅ Base model loaded: dreamshaper_8.safetensors (1.99 GB)\n",
        "\n"
      ],
      "metadata": {
        "id": "P_iaMFEsOtay",
        "outputId": "ae3e54ca-30ec-454f-93d9-4271ee3a9e40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '✅' (U+2705) (ipython-input-4176933677.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-4176933677.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ✅ Base model loaded: dreamshaper_8.safetensors (1.99 GB)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '✅' (U+2705)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Loras/playful_origins_v4/ -lh\n"
      ],
      "metadata": {
        "id": "cVVmugcYEDjF",
        "outputId": "25e7940b-8123-498a-b2b5-40ff39053afd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2.0G\n",
            "drwx------ 2 root root 4.0K Oct 10 06:48 captions\n",
            "drwx------ 2 root root 4.0K Oct  9 05:08 dataset\n",
            "-rw------- 1 root root 2.0G Oct 12 06:56 dreamshaper_8.safetensors\n",
            "drwx------ 2 root root 4.0K Oct  9 04:35 output\n",
            "drwx------ 2 root root 4.0K Oct 11 05:53 output_lora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "caption_dir = \"/content/drive/MyDrive/Loras/playful_origins_v4/captions\"\n",
        "\n",
        "for file in glob.glob(f\"{caption_dir}/*.txt\"):\n",
        "    with open(file, \"r+\", encoding=\"utf-8\") as f:\n",
        "        content = f.read().strip()\n",
        "        if not content.lower().startswith(\"playful_origins_v4\"):\n",
        "            f.seek(0, 0)\n",
        "            f.write(f\"playful_origins_v4, {content}\\n\")\n"
      ],
      "metadata": {
        "id": "HyB9SANeYbay"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 2 /content/drive/MyDrive/Loras/playful_origins_v4/captions/*.txt | head -n 15\n"
      ],
      "metadata": {
        "id": "AU4N9knhYiuv",
        "outputId": "5204e390-d00f-4eff-84ef-95869a4f19aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> /content/drive/MyDrive/Loras/playful_origins_v4/captions/playful_origins_10.txt <==\n",
            "playful_origins_v4, front-facing beauty shot, natural color, true skin tones, no stylization, voluptuous body, natural perky E-cup breasts, vibrant auburn hair, piercing natural green eyes, soft realistic skin texture, accurate proportions, no six-pack, no extreme slimming\n",
            "\n",
            "==> /content/drive/MyDrive/Loras/playful_origins_v4/captions/playful_origins_11.txt <==\n",
            "playful_origins_v4, front face headshot, neutral expression, clean studio light, hair softly framing face, eyes centered and sharp, likeness locked, voluptuous body, natural perky E-cup breasts, vibrant auburn hair, piercing natural green eyes, soft realistic skin texture, accurate proportions, no six-pack, no extreme slimming\n",
            "\n",
            "==> /content/drive/MyDrive/Loras/playful_origins_v4/captions/playful_origins_12.txt <==\n",
            "playful_origins_v4, three-quarter face angle, gentle smile, cheek contour realistic, voluptuous body, natural perky E-cup breasts, vibrant auburn hair, piercing natural green eyes, soft realistic skin texture, accurate proportions, no six-pack, no extreme slimming\n",
            "\n",
            "==> /content/drive/MyDrive/Loras/playful_origins_v4/captions/playful_origins_13.txt <==\n",
            "playful_origins_v4, three-quarter body crop, arms comfortable, clean neckline, realistic cloth folds, voluptuous body, natural perky E-cup breasts, vibrant auburn hair, piercing natural green eyes, soft realistic skin texture, accurate proportions, no six-pack, no extreme slimming\n",
            "\n",
            "==> /content/drive/MyDrive/Loras/playful_origins_v4/captions/playful_origins_14.txt <==\n",
            "playful_origins_v4, torso portrait, soft S-curve posture, correct shoulder width, voluptuous body, natural perky E-cup breasts, vibrant auburn hair, piercing natural green eyes, soft realistic skin texture, accurate proportions, no six-pack, no extreme slimming\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8clWTZEu-g"
      },
      "source": [
        "### ⭕ Disclaimer\n",
        "The purpose of this document is to research bleeding-edge technologies in the field of machine learning.  \n",
        "Please read and follow the [Google Colab guidelines](https://research.google.com/colaboratory/faq.html) and its [Terms of Service](https://research.google.com/colaboratory/tos_v3.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPQlB4djNm3C"
      },
      "source": [
        "| |GitHub|🇬🇧 English|🇪🇸 Spanish|\n",
        "|:--|:-:|:-:|:-:|\n",
        "| 🏠 **Homepage** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab) | | |\n",
        "| 📊 **Dataset Maker** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Dataset_Maker.ipynb) |\n",
        "| ⭐ **Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb) |\n",
        "| 🌟 **XL Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) |  |\n",
        "| 🌟 **Legacy XL Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL_Legacy.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL_Legacy.ipynb) |  |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OglZzI_ujZq-",
        "outputId": "7fa4fc53-8cd1-418f-eb8b-0727e03da06f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using custom base model: /content/drive/MyDrive/Loras/playful_origins_v4/dreamshaper_8.safetensors\n",
            "📝 (Re)building configuration files with LoRA parameters...\n",
            "✅ training_config.toml created with compatible scheduler!\n",
            "✅ dataset_config.toml updated!\n",
            "\n",
            "⭐ Starting trainer...\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
            "/usr/local/lib/python3.12/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
            "/usr/local/lib/python3.12/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760257949.411115   20459 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760257949.417490   20459 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760257949.433935   20459 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760257949.433966   20459 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760257949.433970   20459 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760257949.433975   20459 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "/usr/local/lib/python3.12/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "Loading settings from /content/drive/MyDrive/Loras/playful_origins_v4/training_config.toml...\n",
            "/content/drive/MyDrive/Loras/playful_origins_v4/training_config\n",
            "prepare tokenizer\n",
            "Loading dataset config from /content/drive/MyDrive/Loras/playful_origins_v4/dataset_config.toml\n",
            "prepare images.\n",
            "found directory /content/drive/MyDrive/Loras/playful_origins_v4/dataset contains 73 image files\n",
            "No caption file found for 73 images. Training will continue without captions for these images. If class token exists, it will be used. / 73枚の画像にキャプションファイルが見つかりませんでした。これらの画像についてはキャプションなしで学習を続行します。class tokenが存在する場合はそれを使います。\n",
            "/content/drive/MyDrive/Loras/playful_origins_v4/dataset/playful_origins_10.jpg\n",
            "/content/drive/MyDrive/Loras/playful_origins_v4/dataset/playful_origins_11.png\n",
            "/content/drive/MyDrive/Loras/playful_origins_v4/dataset/playful_origins_12.png\n",
            "/content/drive/MyDrive/Loras/playful_origins_v4/dataset/playful_origins_13.jpg\n",
            "/content/drive/MyDrive/Loras/playful_origins_v4/dataset/playful_origins_14.png\n",
            "/content/drive/MyDrive/Loras/playful_origins_v4/dataset/playful_origins_15.jpg... and 68 more\n",
            "730 train images with repeating.\n",
            "0 reg images.\n",
            "no regularization images / 正則化画像が見つかりませんでした\n",
            "[Dataset 0]\n",
            "  batch_size: 2\n",
            "  resolution: (512, 512)\n",
            "  enable_bucket: True\n",
            "  network_multiplier: 1.0\n",
            "  min_bucket_reso: 256\n",
            "  max_bucket_reso: 1024\n",
            "  bucket_reso_steps: 64\n",
            "  bucket_no_upscale: False\n",
            "\n",
            "  [Subset 0 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/Loras/playful_origins_v4/dataset\"\n",
            "    image_count: 73\n",
            "    num_repeats: 10\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 1\n",
            "    keep_tokens_separator: \n",
            "    caption_separator: ,\n",
            "    secondary_separator: None\n",
            "    enable_wildcard: False\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    caption_prefix: None\n",
            "    caption_suffix: None\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    alpha_mask: False,\n",
            "    is_reg: False\n",
            "    class_tokens: playful_origins_v4\n",
            "    caption_extension: .txt\n",
            "\n",
            "\n",
            "[Dataset 0]\n",
            "loading image sizes.\n",
            "100% 73/73 [00:00<00:00, 488.94it/s]\n",
            "make buckets\n",
            "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
            "bucket 0: resolution (448, 576), count: 20\n",
            "bucket 1: resolution (512, 512), count: 590\n",
            "bucket 2: resolution (576, 448), count: 70\n",
            "bucket 3: resolution (640, 384), count: 40\n",
            "bucket 4: resolution (704, 320), count: 10\n",
            "mean ar error (without repeats): 0.06551328451472653\n",
            "preparing accelerator\n",
            "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:439: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "accelerator device: cuda\n",
            "loading model for process 0/1\n",
            "load StableDiffusion checkpoint: /content/drive/MyDrive/Loras/playful_origins_v4/dreamshaper_8.safetensors\n",
            "UNet2DConditionModel: 64, 8, 768, False, False\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "loading text encoder: <All keys matched successfully>\n",
            "Enable xformers for U-Net\n",
            "import network module: networks.lora\n",
            "[Dataset 0]\n",
            "caching latents.\n",
            "checking cache validity...\n",
            "100% 73/73 [00:00<00:00, 609928.67it/s]\n",
            "caching latents...\n",
            "100% 73/73 [00:32<00:00,  2.27it/s]\n",
            "create LoRA network. base dim (rank): 128, alpha: 128\n",
            "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
            "create LoRA for Text Encoder:\n",
            "create LoRA for Text Encoder: 72 modules.\n",
            "create LoRA for U-Net: 192 modules.\n",
            "enable LoRA for text encoder: 72 modules\n",
            "enable LoRA for U-Net: 192 modules\n",
            "prepare optimizer, data loader etc.\n",
            "use 8-bit AdamW optimizer | {}\n",
            "override steps. steps for 10 epochs is / 指定エポックまでのステップ数: 3650\n",
            "running training / 学習開始\n",
            "  num train images * repeats / 学習画像の数×繰り返し回数: 730\n",
            "  num reg images / 正則化画像の数: 0\n",
            "  num batches per epoch / 1epochのバッチ数: 365\n",
            "  num epochs / epoch数: 10\n",
            "  batch size per device / バッチサイズ: 2\n",
            "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
            "  total optimization steps / 学習ステップ数: 3650\n",
            "steps:   0% 0/3650 [00:00<?, ?it/s]\n",
            "epoch 1/10\n",
            "epoch is incremented. current_epoch: 0, epoch: 1\n",
            "epoch is incremented. current_epoch: 0, epoch: 1\n",
            "steps:  10% 365/3650 [04:37<41:40,  1.31it/s, avr_loss=0.103]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/playful_origins_v4/output/playful_origins_v4-01.safetensors\n",
            "\n",
            "epoch 2/10\n",
            "epoch is incremented. current_epoch: 0, epoch: 2\n",
            "epoch is incremented. current_epoch: 0, epoch: 2\n",
            "steps:  20% 730/3650 [09:17<37:08,  1.31it/s, avr_loss=0.0902]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/playful_origins_v4/output/playful_origins_v4-02.safetensors\n",
            "\n",
            "epoch 3/10\n",
            "epoch is incremented. current_epoch: 0, epoch: 3\n",
            "epoch is incremented. current_epoch: 0, epoch: 3\n",
            "steps:  30% 1095/3650 [13:56<32:31,  1.31it/s, avr_loss=0.0989]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/playful_origins_v4/output/playful_origins_v4-03.safetensors\n",
            "\n",
            "epoch 4/10\n",
            "epoch is incremented. current_epoch: 0, epoch: 4\n",
            "epoch is incremented. current_epoch: 0, epoch: 4\n",
            "steps:  40% 1460/3650 [18:35<27:53,  1.31it/s, avr_loss=0.0935]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/playful_origins_v4/output/playful_origins_v4-04.safetensors\n",
            "\n",
            "epoch 5/10\n",
            "epoch is incremented. current_epoch: 0, epoch: 5\n",
            "epoch is incremented. current_epoch: 0, epoch: 5\n",
            "steps:  50% 1825/3650 [23:15<23:15,  1.31it/s, avr_loss=0.091]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/playful_origins_v4/output/playful_origins_v4-05.safetensors\n",
            "\n",
            "epoch 6/10\n",
            "epoch is incremented. current_epoch: 0, epoch: 6\n",
            "epoch is incremented. current_epoch: 0, epoch: 6\n",
            "steps:  60% 2190/3650 [27:54<18:36,  1.31it/s, avr_loss=0.0901]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/playful_origins_v4/output/playful_origins_v4-06.safetensors\n",
            "\n",
            "epoch 7/10\n",
            "epoch is incremented. current_epoch: 0, epoch: 7\n",
            "epoch is incremented. current_epoch: 0, epoch: 7\n",
            "steps:  70% 2555/3650 [32:32<13:56,  1.31it/s, avr_loss=0.087] \n",
            "saving checkpoint: /content/drive/MyDrive/Loras/playful_origins_v4/output/playful_origins_v4-07.safetensors\n",
            "\n",
            "epoch 8/10\n",
            "epoch is incremented. current_epoch: 0, epoch: 8\n",
            "epoch is incremented. current_epoch: 0, epoch: 8\n",
            "steps:  80% 2920/3650 [37:11<09:17,  1.31it/s, avr_loss=0.0911]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/playful_origins_v4/output/playful_origins_v4-08.safetensors\n",
            "\n",
            "epoch 9/10\n",
            "epoch is incremented. current_epoch: 0, epoch: 9\n",
            "epoch is incremented. current_epoch: 0, epoch: 9\n",
            "steps:  90% 3285/3650 [41:50<04:38,  1.31it/s, avr_loss=0.0886]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/playful_origins_v4/output/playful_origins_v4-09.safetensors\n",
            "\n",
            "epoch 10/10\n",
            "epoch is incremented. current_epoch: 0, epoch: 10\n",
            "epoch is incremented. current_epoch: 0, epoch: 10\n",
            "steps: 100% 3650/3650 [46:30<00:00,  1.31it/s, avr_loss=0.0866]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/playful_origins_v4/output/playful_origins_v4-10.safetensors\n",
            "model saved.\n",
            "steps: 100% 3650/3650 [46:31<00:00,  1.31it/s, avr_loss=0.0866]\n",
            "\n",
            "\u001b[1m✅ Done! Go download your Lora from Google Drive.\n",
            "There will be several files, you should try the latest version (the file with the largest number next to it)\n"
          ]
        }
      ],
      "source": [
        "import os, toml\n",
        "from time import time\n",
        "\n",
        "# --- constants ---\n",
        "COLAB = True\n",
        "project_name = \"playful_origins_v4\"\n",
        "model_file = \"/content/drive/MyDrive/Loras/playful_origins_v4/dreamshaper_8.safetensors\"\n",
        "\n",
        "print(f\"✅ Using custom base model: {model_file}\")\n",
        "\n",
        "root_dir = \"/content\"\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "main_dir = os.path.join(root_dir, \"drive/MyDrive/Loras\")\n",
        "config_folder = os.path.join(main_dir, project_name)\n",
        "log_folder = os.path.join(main_dir, \"_logs\")\n",
        "images_folder = os.path.join(config_folder, \"dataset\")\n",
        "output_folder = os.path.join(config_folder, \"output\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "# --- ensure folders exist ---\n",
        "for d in (main_dir, log_folder, config_folder, images_folder, output_folder):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# --- create proper configs with compatible scheduler ---\n",
        "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "\n",
        "print(\"📝 (Re)building configuration files with LoRA parameters...\")\n",
        "\n",
        "training_dict = {\n",
        "    \"additional_network_arguments\": {\n",
        "        \"unet_lr\": 1e-4,\n",
        "        \"text_encoder_lr\": 5e-5,\n",
        "        \"network_dim\": 128,\n",
        "        \"network_alpha\": 128,\n",
        "        \"network_module\": \"networks.lora\",\n",
        "    },\n",
        "    \"optimizer_arguments\": {\n",
        "        \"optimizer_type\": \"AdamW8bit\",\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"lr_scheduler\": \"cosine_with_restarts\",  # ✅ safe for your kohya version\n",
        "        \"lr_scheduler_num_cycles\": 3,\n",
        "        \"lr_warmup_steps\": 0,\n",
        "    },\n",
        "    \"training_arguments\": {\n",
        "        \"max_train_epochs\": 10,\n",
        "        \"train_batch_size\": 2,\n",
        "        \"save_every_n_epochs\": 1,\n",
        "        \"save_last_n_epochs\": 10,\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"mixed_precision\": \"fp16\",\n",
        "        \"output_dir\": output_folder,\n",
        "        \"logging_dir\": log_folder,\n",
        "        \"output_name\": project_name,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"clip_skip\": 2,\n",
        "        \"xformers\": True,\n",
        "        \"lowram\": True,\n",
        "    },\n",
        "    \"model_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": model_file,\n",
        "        \"v2\": False,\n",
        "    },\n",
        "    \"saving_arguments\": {\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "    },\n",
        "    \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "    },\n",
        "}\n",
        "with open(config_file, \"w\") as f:\n",
        "    toml.dump(training_dict, f)\n",
        "print(\"✅ training_config.toml created with compatible scheduler!\")\n",
        "\n",
        "dataset_dict = {\n",
        "    \"general\": {\n",
        "        \"resolution\": 512,\n",
        "        \"shuffle_caption\": True,\n",
        "        \"keep_tokens\": 1,\n",
        "        \"flip_aug\": False,\n",
        "        \"caption_extension\": \".txt\",\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "        \"min_bucket_reso\": 256,\n",
        "        \"max_bucket_reso\": 1024,\n",
        "    },\n",
        "    \"datasets\": [\n",
        "        {\n",
        "            \"subsets\": [\n",
        "                {\n",
        "                    \"num_repeats\": 10,\n",
        "                    \"image_dir\": images_folder,\n",
        "                    \"class_tokens\": project_name,\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "with open(dataset_config_file, \"w\") as f:\n",
        "    toml.dump(dataset_dict, f)\n",
        "print(\"✅ dataset_config.toml updated!\")\n",
        "\n",
        "# --- start training ---\n",
        "print(\"\\n⭐ Starting trainer...\\n\")\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "!accelerate launch --config_file={accelerate_config_file} \\\n",
        "  --num_cpu_threads_per_process=1 \\\n",
        "  train_network_wrapper.py \\\n",
        "  --dataset_config={dataset_config_file} \\\n",
        "  --config_file={config_file}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBMUJ7BuvNcn"
      },
      "source": [
        "## *️⃣ Extras\n",
        "\n",
        "You can run these before starting the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "sy9jU2yrdYar"
      },
      "outputs": [],
      "source": [
        "#@markdown ### 🔮 Optimizer\n",
        "#@markdown If you run this cell you will change the optimizer used for training. The default will be `AdamW8bit` otherwise, which is recommended.<p>\n",
        "#@markdown * Dadapt and Prodigy manage learning rate automatically and are very good for small datasets. You can use it without changing anything else here.<p>\n",
        "#@markdown For Dadapt and Prodigy, the following values will be overriden if the box is checked:<p>\n",
        "#@markdown `learning_rate=0.5`, `network_alpha=network_dim`, `lr_scheduler=\"constant_with_warmup\"`, `lr_warmup_ratio=0.05`<p>\n",
        "#@markdown For Dadapt and Prodigy, if `optimizer_args` is left empty the default will be `decouple=True, weight_decay=0.01, betas=[0.9,0.999]`<p>\n",
        "#@markdown And additionally for Prodigy: `d_coef=2, use_bias_correction=True, safeguard_warmup=True`<p>\n",
        "optimizer = \"AdamW8bit\" #@param [\"AdamW8bit\", \"Prodigy\", \"DAdaptation\", \"DadaptAdam\", \"DadaptLion\", \"AdamW\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"AdaFactor\"]\n",
        "optimizer_args = \"\" #@param {type:\"string\"}\n",
        "splitter = \", \" if \", \" in optimizer_args else \",\"\n",
        "optimizer_args = [a.strip() for a in optimizer_args.split(splitter) if a]\n",
        "override_values_for_dadapt_and_prodigy = False #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd4916Eu1tb9"
      },
      "source": [
        "### 📚 Multiple folders in dataset\n",
        "Below is a template allowing you to define multiple folders in your dataset. You must include the location of each folder and you can set different number of repeats for each one. To add more folders simply copy and paste the sections starting with `[[datasets.subsets]]`.\n",
        "\n",
        "When enabling this, the number of repeats set in the main cell will be ignored, and the main folder set by the project name will also be ignored.\n",
        "\n",
        "You can make one of them a regularization folder by adding `is_reg = true`  \n",
        "You can also set different `keep_tokens`, `flip_aug`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y037lagnJWmn"
      },
      "outputs": [],
      "source": [
        "custom_dataset = \"\"\"\n",
        "[[datasets]]\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/good_images\"\n",
        "num_repeats = 3\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/normal_images\"\n",
        "num_repeats = 1\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W84Jxf-U2TIU"
      },
      "outputs": [],
      "source": [
        "custom_dataset = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-Yq5mNvcCy2l"
      },
      "outputs": [],
      "source": [
        "#@markdown ### 🤓 Other\n",
        "#@markdown These are kept here for the few people that use them.\n",
        "\n",
        "#@markdown Weighted captions is a new feature that allows you to use (parentheses) to give more weight to certain tags in your dataset, same as in your webui prompts. <p>\n",
        "#@markdown Normal parentheses in your tags such as `(series names)` will need to be escaped like `\\(series names\\)`\n",
        "weighted_captions = False #@param {type:\"boolean\"}\n",
        "\n",
        "#markdown By enabling `adjust_tags`, you will let this colab modify your tags before running to automatically adjust to `weighted_captions` being on or off. <p>\n",
        "#markdown Then, you may increase `activation_tag_weight` to improve how effective your activation tag is.\n",
        "adjust_tags = False #param {type:\"boolean\"}\n",
        "activation_tag_weight = \"1.0\" #param [\"1.0\",\"1.1\",\"1.2\"]\n",
        "keep_tokens_weight = float(activation_tag_weight)\n",
        "\n",
        "#@markdown Here you can write a path in your Google Drive to load an existing Lora file to continue training on.<p>\n",
        "#@markdown **Warning:** It's not the same as one long training session. The epochs start from scratch, and it may have worse results.\n",
        "continue_from_lora = \"\" #@param {type:\"string\"}\n",
        "if continue_from_lora and not continue_from_lora.startswith(\"/content/drive/MyDrive\"):\n",
        "  import os\n",
        "  continue_from_lora = os.path.join(\"/content/drive/MyDrive\", continue_from_lora)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WDjkp4scvPgE"
      },
      "outputs": [],
      "source": [
        "#@markdown ### 📂 Unzip dataset\n",
        "#@markdown It's much slower to upload individual files to your Drive, so you may want to upload a zip if you have your dataset in your computer.\n",
        "zip = \"/content/drive/MyDrive/my_dataset.zip\" #@param {type:\"string\"}\n",
        "extract_to = \"/content/drive/MyDrive/Loras/example/dataset\" #@param {type:\"string\"}\n",
        "\n",
        "import os, zipfile\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  print(\"📂 Connecting to Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip, 'r') as f:\n",
        "  f.extractall(extract_to)\n",
        "\n",
        "print(\"✅ Done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKWlpsG0jrX3"
      },
      "outputs": [],
      "source": [
        "#@markdown ### 🔢 Count datasets\n",
        "#@markdown Google Drive makes it impossible to count the files in a folder, so this will show you the file counts in all folders and subfolders.\n",
        "folder = \"/content/drive/MyDrive/Loras\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"📂 Connecting to Google Drive...\\n\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "tree = {}\n",
        "exclude = (\"_logs\", \"/output\")\n",
        "for i, (root, dirs, files) in enumerate(os.walk(folder, topdown=True)):\n",
        "  dirs[:] = [d for d in dirs if all(ex not in d for ex in exclude)]\n",
        "  images = len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "  captions = len([f for f in files if f.lower().endswith(\".txt\")])\n",
        "  others = len(files) - images - captions\n",
        "  path = root[folder.rfind(\"/\")+1:]\n",
        "  tree[path] = None if not images else f\"{images:>4} images | {captions:>4} captions |\"\n",
        "  if tree[path] and others:\n",
        "    tree[path] += f\" {others:>4} other files\"\n",
        "\n",
        "pad = max(len(k) for k in tree)\n",
        "print(\"\\n\".join(f\"📁{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDyqB2ytNN08"
      },
      "source": [
        "# 📈 Plot training results\n",
        "You can do this after running the trainer. You don't need this unless you know what you're doing.  \n",
        "The first cell below may fail to load all your logs. Keep trying the second cell until all data has loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdogfLJ_NN08"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir={log_folder}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NC0QOaXNN08"
      },
      "outputs": [],
      "source": [
        "from tensorboard import notebook\n",
        "notebook.display(port=6006, height=800)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}